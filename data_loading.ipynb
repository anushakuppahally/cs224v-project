{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data processing functions\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import json\n",
    "import h5py\n",
    "from typing import List, Dict, Any\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from together import Together\n",
    "import streamlit as st\n",
    "import faiss\n",
    "\n",
    "# Data loading and processing functions (keeping previous functions)\n",
    "def load_election_dataset():\n",
    "    \"\"\"Load and initialize the CCNews dataset for 2020 election coverage\"\"\"\n",
    "    dataset = load_dataset(\n",
    "        \"stanford-oval/ccnews\", \n",
    "        name=\"2020\",\n",
    "        split=\"train\", \n",
    "        streaming=True\n",
    "    ).filter(lambda article: article[\"language\"] in [\"en\", \"es\"])\n",
    "    return dataset\n",
    "\n",
    "# [Previous functions remain the same...]\n",
    "def filter_articles(article):\n",
    "    \"\"\"Filter articles to keep only election-related content from 2020\"\"\"\n",
    "    # Check if article has required fields\n",
    "    required_fields = [\"plain_text\", \"published_date\", \"language\"]\n",
    "    if not all(field in article for field in required_fields):\n",
    "        return False\n",
    "        \n",
    "    # Parse date\n",
    "    try:\n",
    "        date = datetime.datetime.strptime(article[\"published_date\"], \"%Y-%m-%d\")\n",
    "        if date.year != 2020:\n",
    "            return False\n",
    "    except:\n",
    "        return False\n",
    "        \n",
    "    # Check language\n",
    "    if article[\"language\"] not in [\"en\", \"es\"]:\n",
    "        return False\n",
    "        \n",
    "    # Check for election-related keywords\n",
    "    keywords = [\"election\", \"vote\", \"voting\", \"Trump\", \"Biden\", \"campaign\", \n",
    "               \"elección\", \"voto\", \"votar\", \"campaña\"]\n",
    "    text = article[\"plain_text\"].lower()\n",
    "    if not any(kw.lower() in text for kw in keywords):\n",
    "        return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "def process_articles(dataset):\n",
    "    \"\"\"Process filtered articles into language-specific collections\"\"\"\n",
    "    articles = {\"en\": [], \"es\": []}\n",
    "    article_id = 0\n",
    "    \n",
    "    for article in dataset:\n",
    "        processed = {\n",
    "            \"id\": article_id,\n",
    "            \"text\": article[\"plain_text\"],\n",
    "            \"date\": article[\"published_date\"],\n",
    "            \"url\": article[\"requested_url\"]\n",
    "        }\n",
    "        articles[article[\"language\"]].append(processed)\n",
    "        article_id += 1\n",
    "        \n",
    "    return articles\n",
    "\n",
    "def generate_embeddings(articles, model_name=\"intfloat/multilingual-e5-large\"):\n",
    "    \"\"\"Generate embeddings for articles using multilingual E5 model\"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = {\"en\": {\"embeddings\": [], \"article_ids\": []},\n",
    "                 \"es\": {\"embeddings\": [], \"article_ids\": []}}\n",
    "                 \n",
    "    for lang in articles:\n",
    "        # Prepare texts with prefix for better retrieval performance\n",
    "        texts = [f\"passage: {article['text']}\" for article in articles[lang]]\n",
    "        # Generate embeddings in batches to manage memory\n",
    "        batch_size = 32\n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            embs = model.encode(batch, normalize_embeddings=True)\n",
    "            all_embeddings.append(embs)\n",
    "            \n",
    "        embeddings[lang][\"embeddings\"] = np.vstack(all_embeddings)\n",
    "        embeddings[lang][\"article_ids\"] = [art[\"id\"] for art in articles[lang]]\n",
    "        \n",
    "    return embeddings\n",
    "\n",
    "def save_embeddings(embeddings, save_dir):\n",
    "    \"\"\"Save embeddings and metadata to disk\"\"\"\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    for lang in embeddings:\n",
    "        with h5py.File(save_dir / f\"{lang}_embeddings.h5\", \"w\") as f:\n",
    "            f.create_dataset(\"embeddings\", data=embeddings[lang][\"embeddings\"])\n",
    "            f.create_dataset(\"article_ids\", data=embeddings[lang][\"article_ids\"])\n",
    "\n",
    "def load_embeddings(load_dir):\n",
    "    \"\"\"Load embeddings and metadata from disk\"\"\"\n",
    "    load_dir = Path(load_dir)\n",
    "    embeddings = {}\n",
    "    \n",
    "    for lang in [\"en\", \"es\"]:\n",
    "        with h5py.File(load_dir / f\"{lang}_embeddings.h5\", \"r\") as f:\n",
    "            embeddings[lang] = {\n",
    "                \"embeddings\": f[\"embeddings\"][:],\n",
    "                \"article_ids\": f[\"article_ids\"][:]\n",
    "            }\n",
    "            \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# New RAG and QA System functions\n",
    "class ElectionQASystem:\n",
    "    def __init__(self, embeddings_dir: str, articles_file: str, model_name: str = \"BAAI/bge-large-en\"):\n",
    "        self.embeddings_dir = Path(embeddings_dir)\n",
    "        self.articles_file = Path(articles_file)\n",
    "        self.embedding_model = SentenceTransformer(model_name)\n",
    "        self.together_client = Together()\n",
    "        self.load_data()\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load embeddings and articles\"\"\"\n",
    "        # Load articles\n",
    "        with open(self.articles_file) as f:\n",
    "            self.articles = json.load(f)\n",
    "            \n",
    "        # Load embeddings and create FAISS indices\n",
    "        self.indices = {}\n",
    "        embeddings = load_embeddings(self.embeddings_dir)\n",
    "        \n",
    "        for lang in embeddings:\n",
    "            index = faiss.IndexFlatL2(embeddings[lang][\"embeddings\"].shape[1])\n",
    "            index.add(embeddings[lang][\"embeddings\"])\n",
    "            self.indices[lang] = {\n",
    "                \"index\": index,\n",
    "                \"article_ids\": embeddings[lang][\"article_ids\"]\n",
    "            }\n",
    "    \n",
    "    def get_relevant_context(self, query: str, language: str, k: int = 3) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Retrieve relevant articles using RAG\"\"\"\n",
    "        # Encode query\n",
    "        query_embedding = self.embedding_model.encode([query])[0]\n",
    "        \n",
    "        # Search in appropriate language index\n",
    "        D, I = self.indices[language][\"index\"].search(\n",
    "            np.array([query_embedding]), k\n",
    "        )\n",
    "        \n",
    "        # Get relevant articles\n",
    "        relevant_articles = []\n",
    "        for idx in I[0]:\n",
    "            article_id = self.indices[language][\"article_ids\"][idx]\n",
    "            article = next(\n",
    "                art for art in self.articles[language] \n",
    "                if art[\"id\"] == article_id\n",
    "            )\n",
    "            relevant_articles.append(article)\n",
    "            \n",
    "        return relevant_articles\n",
    "    \n",
    "    def generate_answer(self, query: str, context: List[Dict[str, Any]], language: str) -> str:\n",
    "        \"\"\"Generate answer using LLM\"\"\"\n",
    "        # Format context\n",
    "        context_text = \"\\n\\n\".join(\n",
    "            f\"Title: {art['title']}\\nContent: {art['text']}\" \n",
    "            for art in context\n",
    "        )\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = f\"\"\"Based on the following articles about the 2020 US Election, please answer the question.\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "        # Generate response using Together API\n",
    "        response = self.together_client.complete(\n",
    "            prompt=prompt,\n",
    "            model=\"togethercomputer/llama-2-70b-chat\",\n",
    "            max_tokens=500,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        return response.output.text\n",
    "\n",
    "# Suggested repository structure:\n",
    "\"\"\"\n",
    "election_qa/\n",
    "├── data/\n",
    "│   ├── raw/                      \n",
    "│   │   └── articles.json         # All articles grouped by language\n",
    "│   └── processed/              \n",
    "│       ├── metadata/           \n",
    "│       │   └── article_index.json\n",
    "│       └── embeddings/          \n",
    "│           ├── embeddings_en.h5  \n",
    "│           └── embeddings_es.h5  \n",
    "├── src/\n",
    "│   ├── data_processing/\n",
    "│   │   ├── __init__.py\n",
    "│   │   └── loader.py            # This file's data loading functions\n",
    "│   ├── qa/\n",
    "│   │   ├── __init__.py\n",
    "│   │   └── system.py            # ElectionQASystem class\n",
    "│   └── ui/\n",
    "│       ├── __init__.py\n",
    "│       └── app.py               # Streamlit interface\n",
    "├── requirements.txt\n",
    "└── README.md\n",
    "\"\"\"\n",
    "\n",
    "# Streamlit UI code (src/ui/app.py):\n",
    "def create_streamlit_app():\n",
    "    st.title(\"2020 Election Q&A System\")\n",
    "    \n",
    "    # Initialize QA system\n",
    "    qa_system = ElectionQASystem(\n",
    "        embeddings_dir=\"data/processed/embeddings\",\n",
    "        articles_file=\"data/raw/articles.json\"\n",
    "    )\n",
    "    \n",
    "    # Language selection\n",
    "    language = st.selectbox(\n",
    "        \"Select Language\",\n",
    "        options=[\"en\", \"es\"],\n",
    "        format_func=lambda x: \"English\" if x == \"en\" else \"Spanish\"\n",
    "    )\n",
    "    \n",
    "    # Query input\n",
    "    query = st.text_input(\"Enter your question about the 2020 US Election:\")\n",
    "    \n",
    "    if query:\n",
    "        with st.spinner(\"Searching for relevant information...\"):\n",
    "            # Get relevant context\n",
    "            context = qa_system.get_relevant_context(query, language)\n",
    "            \n",
    "            # Generate answer\n",
    "            answer = qa_system.generate_answer(query, context, language)\n",
    "            \n",
    "            # Display results\n",
    "            st.subheader(\"Answer:\")\n",
    "            st.write(answer)\n",
    "            \n",
    "            st.subheader(\"Sources:\")\n",
    "            for article in context:\n",
    "                with st.expander(f\"Source: {article['source']} - {article['date']}\"):\n",
    "                    st.write(f\"**{article['title']}**\")\n",
    "                    st.write(article['text'][:500] + \"...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # For data processing script\n",
    "    dataset = load_election_dataset()\n",
    "    \n",
    "    # Define filters\n",
    "    start_date = datetime.datetime(2020, 11, 4, tzinfo=datetime.timezone.utc)\n",
    "    end_date = datetime.datetime(2020, 11, 5, tzinfo=datetime.timezone.utc)\n",
    "    keywords = [\"election\", \"presidential\", \"Biden\", \"Trump\", \"vote\", \"elections\"]\n",
    "    \n",
    "    # Process and save data\n",
    "    processed_articles = []\n",
    "    for article in dataset:\n",
    "        if filter_by_date(article, start_date, end_date):\n",
    "            if filter_by_keywords(article.get(\"title\", \"\") + article.get(\"plain_text\", \"\"), keywords):\n",
    "                processed_article = process_article(article)\n",
    "                processed_articles.append(processed_article)\n",
    "    \n",
    "    save_processed_data(processed_articles, \"data/raw/articles.json\")\n",
    "    \n",
    "    # For Streamlit app\n",
    "    create_streamlit_app()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "import json\n",
    "from together import Together\n",
    "import os\n",
    "import requests\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b15c4710294be09beb032dd62ddb75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/479 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe02bc19f304823928e330c6c9416d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m filtered_dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mfilter(filter_articles)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Iterate over the filtered dataset\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m filtered_dataset:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(example)\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/site-packages/datasets/iterable_dataset.py:2093\u001b[0m, in \u001b[0;36mIterableDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2090\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m formatter\u001b[38;5;241m.\u001b[39mformat_row(pa_table)\n\u001b[1;32m   2091\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m-> 2093\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, example \u001b[38;5;129;01min\u001b[39;00m ex_iterable:\n\u001b[1;32m   2094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ex_iterable\u001b[38;5;241m.\u001b[39mis_typed:\n\u001b[1;32m   2095\u001b[0m         \u001b[38;5;66;03m# `IterableDataset` automatically fills missing columns with None.\u001b[39;00m\n\u001b[1;32m   2096\u001b[0m         \u001b[38;5;66;03m# This is done with `_apply_feature_types_on_example`.\u001b[39;00m\n\u001b[1;32m   2097\u001b[0m         example \u001b[38;5;241m=\u001b[39m _apply_feature_types_on_example(\n\u001b[1;32m   2098\u001b[0m             example, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures, token_per_repo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_per_repo_id\n\u001b[1;32m   2099\u001b[0m         )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/site-packages/datasets/iterable_dataset.py:1260\u001b[0m, in \u001b[0;36mFilteredExamplesIterable.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1258\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m key, formatter\u001b[38;5;241m.\u001b[39mformat_row(pa_table)\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1260\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/site-packages/datasets/iterable_dataset.py:1316\u001b[0m, in \u001b[0;36mFilteredExamplesIterable._iter\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1314\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprevious_state_example_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m current_idx\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1316\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, example \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[1;32m   1317\u001b[0m         \u001b[38;5;66;03m# If not batched, we can apply the filtering function direcly\u001b[39;00m\n\u001b[1;32m   1318\u001b[0m         example \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(example)\n\u001b[1;32m   1319\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m format_dict(example) \u001b[38;5;28;01mif\u001b[39;00m format_dict \u001b[38;5;28;01melse\u001b[39;00m example\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/site-packages/datasets/iterable_dataset.py:1683\u001b[0m, in \u001b[0;36mTypedExamplesIterable.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1681\u001b[0m     \u001b[38;5;66;03m# Then for each example, `TypedExamplesIterable` automatically fills missing columns with None.\u001b[39;00m\n\u001b[1;32m   1682\u001b[0m     \u001b[38;5;66;03m# This is done with `_apply_feature_types_on_example`.\u001b[39;00m\n\u001b[0;32m-> 1683\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, example \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mex_iterable:\n\u001b[1;32m   1684\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m (\n\u001b[1;32m   1685\u001b[0m             key,\n\u001b[1;32m   1686\u001b[0m             _apply_feature_types_on_example(example, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures, token_per_repo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_per_repo_id),\n\u001b[1;32m   1687\u001b[0m         )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/site-packages/datasets/iterable_dataset.py:1260\u001b[0m, in \u001b[0;36mFilteredExamplesIterable.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1258\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m key, formatter\u001b[38;5;241m.\u001b[39mformat_row(pa_table)\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1260\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/site-packages/datasets/iterable_dataset.py:1316\u001b[0m, in \u001b[0;36mFilteredExamplesIterable._iter\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1314\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprevious_state_example_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m current_idx\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1316\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, example \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[1;32m   1317\u001b[0m         \u001b[38;5;66;03m# If not batched, we can apply the filtering function direcly\u001b[39;00m\n\u001b[1;32m   1318\u001b[0m         example \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(example)\n\u001b[1;32m   1319\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m format_dict(example) \u001b[38;5;28;01mif\u001b[39;00m format_dict \u001b[38;5;28;01melse\u001b[39;00m example\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/site-packages/datasets/iterable_dataset.py:1683\u001b[0m, in \u001b[0;36mTypedExamplesIterable.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1681\u001b[0m     \u001b[38;5;66;03m# Then for each example, `TypedExamplesIterable` automatically fills missing columns with None.\u001b[39;00m\n\u001b[1;32m   1682\u001b[0m     \u001b[38;5;66;03m# This is done with `_apply_feature_types_on_example`.\u001b[39;00m\n\u001b[0;32m-> 1683\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, example \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mex_iterable:\n\u001b[1;32m   1684\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m (\n\u001b[1;32m   1685\u001b[0m             key,\n\u001b[1;32m   1686\u001b[0m             _apply_feature_types_on_example(example, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures, token_per_repo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_per_repo_id),\n\u001b[1;32m   1687\u001b[0m         )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/site-packages/datasets/iterable_dataset.py:279\u001b[0m, in \u001b[0;36mArrowExamplesIterable.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m shard_example_idx_start \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshard_example_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_dict \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    278\u001b[0m shard_example_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 279\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, pa_table \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_tables_fn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgen_kwags):\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shard_example_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(pa_table) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m shard_example_idx_start:\n\u001b[1;32m    281\u001b[0m         shard_example_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(pa_table)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/site-packages/datasets/packaged_modules/parquet/parquet.py:86\u001b[0m, in \u001b[0;36mParquet._generate_tables\u001b[0;34m(self, files)\u001b[0m\n\u001b[1;32m     84\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;129;01mor\u001b[39;00m parquet_file\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mrow_group(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnum_rows\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, record_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[1;32m     87\u001b[0m         parquet_file\u001b[38;5;241m.\u001b[39miter_batches(batch_size\u001b[38;5;241m=\u001b[39mbatch_size, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m     88\u001b[0m     ):\n\u001b[1;32m     89\u001b[0m         pa_table \u001b[38;5;241m=\u001b[39m pa\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_batches([record_batch])\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;66;03m# Uncomment for debugging (will print the Arrow table size and elements)\u001b[39;00m\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;66;03m# logger.warning(f\"pa_table: {pa_table} num rows: {pa_table.num_rows}\")\u001b[39;00m\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;66;03m# logger.warning('\\n'.join(str(pa_table.slice(i, 1).to_pydict()) for i in range(pa_table.num_rows)))\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/site-packages/pyarrow/_parquet.pyx:1613\u001b[0m, in \u001b[0;36miter_batches\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/site-packages/pyarrow/error.pxi:89\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/site-packages/datasets/utils/file_utils.py:826\u001b[0m, in \u001b[0;36m_add_retries_to_file_obj_read_method.<locals>.read_with_retries\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m retry \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_retries \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    825\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 826\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    829\u001b[0m         aiohttp\u001b[38;5;241m.\u001b[39mclient_exceptions\u001b[38;5;241m.\u001b[39mClientError,\n\u001b[1;32m    830\u001b[0m         asyncio\u001b[38;5;241m.\u001b[39mTimeoutError,\n\u001b[1;32m    831\u001b[0m         requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mConnectionError,\n\u001b[1;32m    832\u001b[0m         requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mTimeout,\n\u001b[1;32m    833\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/site-packages/huggingface_hub/hf_file_system.py:757\u001b[0m, in \u001b[0;36mHfFileSystemFile.read\u001b[0;34m(self, length)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m, block_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:  \u001b[38;5;66;03m# block_size=0 enables fast streaming\u001b[39;00m\n\u001b[1;32m    756\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 757\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/site-packages/fsspec/spec.py:1941\u001b[0m, in \u001b[0;36mAbstractBufferedFile.read\u001b[0;34m(self, length)\u001b[0m\n\u001b[1;32m   1938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m length \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1939\u001b[0m     \u001b[38;5;66;03m# don't even bother calling fetch\u001b[39;00m\n\u001b[1;32m   1940\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1941\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1943\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m   1944\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m read: \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1945\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1948\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache\u001b[38;5;241m.\u001b[39m_log_stats(),\n\u001b[1;32m   1949\u001b[0m )\n\u001b[1;32m   1950\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(out)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/site-packages/fsspec/caching.py:234\u001b[0m, in \u001b[0;36mReadAheadCache._fetch\u001b[0;34m(self, start, end)\u001b[0m\n\u001b[1;32m    232\u001b[0m end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, end \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_requested_bytes \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m end \u001b[38;5;241m-\u001b[39m start\n\u001b[0;32m--> 234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# new block replaces old\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m=\u001b[39m start\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/site-packages/huggingface_hub/hf_file_system.py:713\u001b[0m, in \u001b[0;36mHfFileSystemFile._fetch_range\u001b[0;34m(self, start, end)\u001b[0m\n\u001b[1;32m    702\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrange\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbytes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39m_api\u001b[38;5;241m.\u001b[39m_build_hf_headers(),\n\u001b[1;32m    705\u001b[0m }\n\u001b[1;32m    706\u001b[0m url \u001b[38;5;241m=\u001b[39m hf_hub_url(\n\u001b[1;32m    707\u001b[0m     repo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolved_path\u001b[38;5;241m.\u001b[39mrepo_id,\n\u001b[1;32m    708\u001b[0m     revision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolved_path\u001b[38;5;241m.\u001b[39mrevision,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    711\u001b[0m     endpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mendpoint,\n\u001b[1;32m    712\u001b[0m )\n\u001b[0;32m--> 713\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mhttp_backoff\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry_on_status_codes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m502\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m503\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m504\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstants\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHF_HUB_DOWNLOAD_TIMEOUT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    720\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/site-packages/huggingface_hub/utils/_http.py:307\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[1;32m    304\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mseek(io_obj_initial_pos)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 307\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m retry_on_status_codes:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/site-packages/requests/sessions.py:724\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_redirects:\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;66;03m# Redirect resolving generator.\u001b[39;00m\n\u001b[1;32m    723\u001b[0m     gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_redirects(r, request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 724\u001b[0m     history \u001b[38;5;241m=\u001b[39m [resp \u001b[38;5;28;01mfor\u001b[39;00m resp \u001b[38;5;129;01min\u001b[39;00m gen]\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    726\u001b[0m     history \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/site-packages/requests/sessions.py:724\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_redirects:\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;66;03m# Redirect resolving generator.\u001b[39;00m\n\u001b[1;32m    723\u001b[0m     gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_redirects(r, request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 724\u001b[0m     history \u001b[38;5;241m=\u001b[39m [resp \u001b[38;5;28;01mfor\u001b[39;00m resp \u001b[38;5;129;01min\u001b[39;00m gen]\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    726\u001b[0m     history \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/site-packages/requests/sessions.py:265\u001b[0m, in \u001b[0;36mSessionRedirectMixin.resolve_redirects\u001b[0;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m req\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 265\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madapter_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     extract_cookies_to_jar(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcookies, prepared_request, resp\u001b[38;5;241m.\u001b[39mraw)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;66;03m# extract redirect url, if any, for the next loop\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/site-packages/requests/sessions.py:746\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[0;32m--> 746\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/site-packages/requests/models.py:902\u001b[0m, in \u001b[0;36mResponse.content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 902\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/site-packages/urllib3/response.py:1060\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1060\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1062\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1063\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/site-packages/urllib3/response.py:949\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    947\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 949\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    951\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/site-packages/urllib3/response.py:873\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    870\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 873\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    875\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    876\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    883\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/site-packages/urllib3/response.py:856\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/http/client.py:459\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;66;03m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[1;32m    458\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(amt)\n\u001b[0;32m--> 459\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b)[:n]\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;66;03m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;66;03m# and self.chunked\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/http/client.py:503\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    498\u001b[0m         b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmemoryview\u001b[39m(b)[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength]\n\u001b[1;32m    500\u001b[0m \u001b[38;5;66;03m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;66;03m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;66;03m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[0;32m--> 503\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m n \u001b[38;5;129;01mand\u001b[39;00m b:\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/socket.py:681\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 681\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    683\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/ssl.py:1132\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = load_dataset( \"stanford-oval/ccnews\", name=\"2020\", split=\"train\", streaming=True ).filter(lambda article: article[\"language\"] in [\"en\", \"es\"])\n",
    "# filtering for 2020 election dates \n",
    "start_date = datetime.datetime(2020, 11, 4, tzinfo=datetime.timezone.utc)\n",
    "end_date = datetime.datetime(2020, 11, 5, tzinfo=datetime.timezone.utc)\n",
    "keywords = [\"election\", \"presidential\", \"Biden\", \"Trump\", \"vote\", \"elections\"]\n",
    "\n",
    "# Define the filter function\n",
    "def filter_articles(article):\n",
    "    try:\n",
    "        # Parse the crawl date using dateutil for flexible formats\n",
    "        crawl_date = parser.isoparse(article[\"crawl_date\"])\n",
    "    except ValueError as e:\n",
    "        # Skip rows with invalid dates\n",
    "        print(f\"Skipping article due to date parsing error: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Check date range\n",
    "    if not (start_date <= crawl_date <= end_date):\n",
    "        return False\n",
    "    \n",
    "    # Check language\n",
    "    # if article[\"language\"] not in [\"en\", \"es\"]:\n",
    "    #     return False\n",
    "    \n",
    "    # Check keywords in title or content\n",
    "    title = article.get(\"title\", \"\").lower()\n",
    "    content = article.get(\"content\", \"\").lower()\n",
    "    if any(keyword in title or keyword in content for keyword in keywords):\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Apply the filter while streaming\n",
    "filtered_dataset = dataset.filter(filter_articles)\n",
    "\n",
    "# Iterate over the filtered dataset\n",
    "for example in filtered_dataset:\n",
    "    print(example)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d11a38cb68634d2eb75c394389ad71de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/479 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ed73a69e0b84b67ab9f4e58fb89174e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnPoolWorker-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/birds/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/envs/birds/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/envs/birds/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/miniconda3/envs/birds/lib/python3.8/multiprocessing/queues.py\", line 358, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'process_chunk' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(cpu_count()) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m stream_in_chunks(dataset, chunk_size\u001b[38;5;241m=\u001b[39mchunk_size):\n\u001b[0;32m---> 60\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m filtered \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[1;32m     62\u001b[0m             filtered_articles\u001b[38;5;241m.\u001b[39mextend(filtered)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/multiprocessing/pool.py:364\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    360\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/multiprocessing/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/threading.py:558\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 558\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/opt/miniconda3/envs/birds/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from datetime import datetime, timezone\n",
    "from dateutil import parser\n",
    "\n",
    "# Define constants\n",
    "start_date = datetime(2020, 11, 4, tzinfo=timezone.utc)\n",
    "end_date = datetime(2020, 11, 5, tzinfo=timezone.utc)\n",
    "keywords = [\"election\", \"presidential\", \"Biden\", \"Trump\", \"vote\", \"elections\"]\n",
    "\n",
    "# Define the filter function\n",
    "def filter_articles(article):\n",
    "    try:\n",
    "        # Parse the crawl date\n",
    "        crawl_date = parser.isoparse(article[\"crawl_date\"])\n",
    "    except ValueError:\n",
    "        # Skip invalid dates\n",
    "        return None\n",
    "    \n",
    "    # Check date range\n",
    "    if not (start_date <= crawl_date <= end_date):\n",
    "        return None\n",
    "    \n",
    "    # Check keywords in title or content\n",
    "    title = article.get(\"title\", \"\").lower()\n",
    "    content = article.get(\"content\", \"\").lower()\n",
    "    if any(keyword in title or keyword in content for keyword in keywords):\n",
    "        return article  # Return the article if it matches\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Helper to process a chunk of articles\n",
    "def process_chunk(chunk):\n",
    "    return [filter_articles(article) for article in chunk if filter_articles(article) is not None]\n",
    "\n",
    "# Function to stream the dataset in chunks\n",
    "def stream_in_chunks(dataset, chunk_size=1000):\n",
    "    buffer = []\n",
    "    for article in dataset:\n",
    "        buffer.append(article)\n",
    "        if len(buffer) == chunk_size:\n",
    "            yield buffer\n",
    "            buffer = []\n",
    "    if buffer:  # Yield the last chunk\n",
    "        yield buffer\n",
    "\n",
    "# Initialize the dataset\n",
    "dataset = load_dataset(\n",
    "    \"stanford-oval/ccnews\", \n",
    "    name=\"2020\", \n",
    "    split=\"train\", \n",
    "    streaming=True\n",
    ").filter(lambda article: article[\"language\"] in [\"en\", \"es\"])\n",
    "\n",
    "# Use multiprocessing to process the dataset in chunks\n",
    "filtered_articles = []\n",
    "chunk_size = 1000\n",
    "with Pool(cpu_count()) as pool:\n",
    "    for chunk in stream_in_chunks(dataset, chunk_size=chunk_size):\n",
    "        results = pool.map(process_chunk, [chunk])\n",
    "        for filtered in results:\n",
    "            filtered_articles.extend(filtered)\n",
    "\n",
    "# Print some filtered articles\n",
    "for article in filtered_articles[:5]:\n",
    "    print(article)\n",
    "\n",
    "# Print some filtered articles\n",
    "# for article in filtered_articles[:5]:\n",
    "#     print(article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c40743f12643418d992021600f2d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/479 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ac27d8136dc4b87b9f5704463c49ad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'requested_url': 'https://www.telez.fr/actus-tv/demain-nous-appartient-en-avance-resume-de-lepisode-629-de-mercredi-1er-janvier/', 'plain_text': 'TF1 diffuse le mercredi 1er janvier l’épisode 629 du feuilleton Demain nous appartient. Au menu : Bart piégé, Samuel s’énerve. Attention spoilers !  Dans Demain nous appartient, Bart se réveille seul dans son lit. Il retrouve sa mère au petit déjeuner. Elle lui demande pardon pour la dispute de la veille. Flore lui redit combien il est important pour elle qu’ils s’entendent bien. Bart lui promet de tout faire pour que son mariage se passe bien. Quelqu’un sonne à la porte : une enveloppe adressée à Bart est posée sur le paillasson. Dedans, on devine des photos compromettantes. Bart va voir son cousin pour lui montrer. Un mot y est associé : s’il continue à fouiller, il aura des problèmes. Il sait qu’Audrey l’a piégé. Max lui conseille de chercher cette Audrey. Ils débarquent au Spoon pour voir Ulysse et lui demander s’il a des infos sur cette Audrey. Mais il ne sait rien de plus. Les cousins se rendent au port pour coller des photocopies des photos pour leur montrer qu’il n’a pas peur de leurs intimidations. Le chef des trafiquants l’observe de loin, et donne des consignes au téléphone pour muscler les intimidations. Un peu plus tard, Audrey arrive au commissariat pour porter plainte contre Bart qui l’aurait violé. Elle sert à Martin et Sarah un témoignage édifiant. Après la déposition, Martin interroge Sarah qui se sent mal et se pose pleins de questions.  Samuel et William en viennent aux mains  Chez Leila, Samuel se renseigne sur ses droits en tant que père biologique. Il envisage une action en justice pour pouvoir approcher Sofia. Leila veut le soutenir dans sa démarche. De son côté, Sofia veut sortir de chez elle et qu’on la laisse tranquille. Elle étouffe. À l’hôpital, Samuel va voir William pour lui dire qu’il en a marre qu’Aurore le menace sans arrêt. Il lui annonce qu’il compte entamer une action en justice : William met un coup de poing à Samuel. Son père Renaud le soigne et Samuel lui raconte. Puis Marianne vient voir Renaud qui lui raconte tout à son tour. Il soutient son fils car Sofia est aussi sa petite-fille, par ricochet.  Grande décision chez les Delcourt  Chez les Delcourt, les deux frères parlent de leur mère qui se voit décliner. Ils parlent de leur idée pour sauver le mas à Chloé : proposer à Yvan de devenir leur associé minoritaire. Alex et Robin le rencontrent mais Yvan n’est pas d’accord avec leur proposition. Dans le salon, Jeanne raconte à Judith qu’elle écrit sa vie pour s’en souvenir. Elles partagent un joli moment. Le soir, Alex rentre et raconte à Chloé et Jeanne qu’Yvan veut tout le mas ou rien. Robin et Alex ont donc décidé de tout vendre. Jeanne s’en veut mais Alex est désemparé.  La dernière séquence de Demain nous appartient  Flore, Arnaud et tous les enfants sont à table lorsque Martin et Sarah sonnent à la porte : Bart est placé en garde à vue pour le viol d’Audrey Richard…  Demain nous appartient, mercredi 1er janvier à 19h20 sur TF1', 'published_date': '2019-12-31', 'title': '\"Demain nous appartient\" en avance : résumé de l\\'épisode 629 de mercredi 1er janvier | News TV Télé Z', 'tags': 'Demain nous appartient', 'categories': 'Feuilleton', 'author': 'La Rédaction', 'sitename': 'TéléZ', 'image_url': 'https://www.telez.fr/content/uploads/2019/12/629.jpg', 'language': 'fr', 'language_score': 0.9897007942199707, 'responded_url': 'https://www.telez.fr/actus-tv/demain-nous-appartient-en-avance-resume-de-lepisode-629-de-mercredi-1er-janvier/', 'publisher': 'telez.fr', 'warc_path': 'https://data.commoncrawl.org/crawl-data/CC-NEWS/2020/01/CC-NEWS-20200101023937-00188.warc.gz', 'crawl_date': '2020-01-01T02:39:37+00:00'}\n"
     ]
    }
   ],
   "source": [
    "iterable_dataset = load_dataset(\"stanford-oval/ccnews\", name=\"2020\", split = \"train\",streaming=True)\n",
    "for example in iterable_dataset:\n",
    "    print(example)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetInfo(description='', citation='', homepage='', license='', features={'requested_url': Value(dtype='string', id=None), 'plain_text': Value(dtype='string', id=None), 'published_date': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'tags': Value(dtype='string', id=None), 'categories': Value(dtype='string', id=None), 'author': Value(dtype='string', id=None), 'sitename': Value(dtype='string', id=None), 'image_url': Value(dtype='string', id=None), 'language': Value(dtype='string', id=None), 'language_score': Value(dtype='float64', id=None), 'responded_url': Value(dtype='string', id=None), 'publisher': Value(dtype='string', id=None), 'warc_path': Value(dtype='string', id=None), 'crawl_date': Value(dtype='string', id=None)}, post_processed=None, supervised_keys=None, builder_name='parquet', dataset_name='ccnews', config_name='2020', version=0.0.0, splits=None, download_checksums=None, download_size=None, post_processing_size=None, dataset_size=None, size_in_bytes=None)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterable_dataset.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize client with API key\n",
    "with open(\"config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "api_key = config[\"api_key\"]\n",
    "# print(api_key)\n",
    "\n",
    "client = Together(api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(input_texts):\n",
    "    \"\"\"Generate embeddings from Together API.\n",
    "\n",
    "    Args:\n",
    "        input_texts: a list of string input texts.\n",
    "        model_api_string: str. An API string for a specific embedding model of your choice.\n",
    "\n",
    "    Returns:\n",
    "        embeddings_list: a list of embeddings. Each element corresponds to the each input text.\n",
    "    \"\"\"\n",
    "    model_api_string = \"togethercomputer/m2-bert-80M-8k-retrieval\"\n",
    "    outputs = client.embeddings.create(\n",
    "        input=input_texts, \n",
    "        model=model_api_string,\n",
    "    )\n",
    "    return [x.embedding for x in outputs.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'requested_url': Value(dtype='string', id=None),\n",
       " 'plain_text': Value(dtype='string', id=None),\n",
       " 'published_date': Value(dtype='string', id=None),\n",
       " 'title': Value(dtype='string', id=None),\n",
       " 'tags': Value(dtype='string', id=None),\n",
       " 'categories': Value(dtype='string', id=None),\n",
       " 'author': Value(dtype='string', id=None),\n",
       " 'sitename': Value(dtype='string', id=None),\n",
       " 'image_url': Value(dtype='string', id=None),\n",
       " 'language': Value(dtype='string', id=None),\n",
       " 'language_score': Value(dtype='float64', id=None),\n",
       " 'responded_url': Value(dtype='string', id=None),\n",
       " 'publisher': Value(dtype='string', id=None),\n",
       " 'warc_path': Value(dtype='string', id=None),\n",
       " 'crawl_date': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'requested_url': 'https://www.telez.fr/actus-tv/demain-nous-appartient-en-avance-resume-de-lepisode-629-de-mercredi-1er-janvier/', 'plain_text': \"TF1 diffuse le mercredi 1er janvier l'épisode 629 du feuilleton Demain nous appartient. Au menu : Bart piégé, Samuel s'énerve. Attention spoilers !  Dans Demain nous appartient, Bart se réveille seul dans son lit. Il retrouve sa mère au petit déjeuner. Elle lui demande pardon pour la dispute de la veille. Flore lui redit combien il est important pour elle qu'ils s'entendent bien. Bart lui promet de tout faire pour que son mariage se passe bien. Quelqu'un sonne à la porte : une enveloppe adressée à Bart est posée sur le paillasson. Dedans, on devine des photos compromettantes. Bart va voir son cousin pour lui montrer. Un mot y est associé : s'il continue à fouiller, il aura des problèmes. Il sait qu'Audrey l'a piégé. Max lui conseille de chercher cette Audrey. Ils débarquent au Spoon pour voir Ulysse et lui demander s'il a des infos sur cette Audrey. Mais il ne sait rien de plus. Les cousins se rendent au port pour coller des photocopies des photos pour leur montrer qu'il n'a pas peur de leurs intimidations. Le chef des trafiquants l'observe de loin, et donne des consignes au téléphone pour muscler les intimidations. Un peu plus tard, Audrey arrive au commissariat pour porter plainte contre Bart qui l'aurait violé. Elle sert à Martin et Sarah un témoignage édifiant. Après la déposition, Martin interroge Sarah qui se sent mal et se pose pleins de questions.  Samuel et William en viennent aux mains  Chez Leila, Samuel se renseigne sur ses droits en tant que père biologique. Il envisage une action en justice pour pouvoir approcher Sofia. Leila veut le soutenir dans sa démarche. De son côté, Sofia veut sortir de chez elle et qu'on la laisse tranquille. Elle étouffe. À l'hôpital, Samuel va voir William pour lui dire qu'il en a marre qu'Aurore le menace sans arrêt. Il lui annonce qu'il compte entamer une action en justice : William met un coup de poing à Samuel. Son père Renaud le soigne et Samuel lui raconte. Puis Marianne vient voir Renaud qui lui raconte tout à son tour. Il soutient son fils car Sofia est aussi sa petite-fille, par ricochet.  Grande décision chez les Delcourt  Chez les Delcourt, les deux frères parlent de leur mère qui se voit décliner. Ils parlent de leur idée pour sauver le mas à Chloé : proposer à Yvan de devenir leur associé minoritaire. Alex et Robin le rencontrent mais Yvan n'est pas d'accord avec leur proposition. Dans le salon, Jeanne raconte à Judith qu'elle écrit sa vie pour s'en souvenir. Elles partagent un joli moment. Le soir, Alex rentre et raconte à Chloé et Jeanne qu'Yvan veut tout le mas ou rien. Robin et Alex ont donc décidé de tout vendre. Jeanne s'en veut mais Alex est désemparé.  La dernière séquence de Demain nous appartient  Flore, Arnaud et tous les enfants sont à table lorsque Martin et Sarah sonnent à la porte : Bart est placé en garde à vue pour le viol d'Audrey Richard…  Demain nous appartient, mercredi 1er janvier à 19h20 sur TF1\", 'published_date': '2019-12-31', 'title': '\"Demain nous appartient\" en avance : résumé de l\\'épisode 629 de mercredi 1er janvier | News TV Télé Z', 'tags': 'Demain nous appartient', 'categories': 'Feuilleton', 'author': 'La Rédaction', 'sitename': 'TéléZ', 'image_url': 'https://www.telez.fr/content/uploads/2019/12/629.jpg', 'language': 'fr', 'language_score': 0.9897007942199707, 'responded_url': 'https://www.telez.fr/actus-tv/demain-nous-appartient-en-avance-resume-de-lepisode-629-de-mercredi-1er-janvier/', 'publisher': 'telez.fr', 'warc_path': 'https://data.commoncrawl.org/crawl-data/CC-NEWS/2020/01/CC-NEWS-20200101023937-00188.warc.gz', 'crawl_date': '2020-01-01T02:39:37+00:00'}, {'requested_url': 'https://www.newyorker.com/newsletter/the-daily/the-lessons-of-trumps-election-triumph', 'plain_text': '“What a night in America. A long, long night,” Vinson Cunningham wrote, as what seemed set to be a tight contest between Donald Trump and Kamala Harris rapidly revealed itself to be a dominant victory for the former President and his fellow-Republicans.', 'published_date': '2019-12-31', 'language': 'en', 'crawl_date': '2024-01-01T02:39:37+00:00'}, {'requested_url': 'https://laopinion.com/2024/11/08/el-sorpendente-voto-latino-por-trump-la-economia-la-inmigracion-y-el-machismo/', 'plain_text': 'Hubo varias señales sobre cómo los latinos se estaban inclinando hacia la propuesta electoral del presidente electo Donald Trump. Las organizaciones civiles que hacen trabajo de campo con votantes latinos todavía analizan los motivos para que aumentara el respaldo a republicanos y a favor, particularmente, del presidente electo Donald Trump.¿Fue la economía, la inmigración? En gran medida fueron esos los motivos que inclinaron el voto de los latinos, pero también “algo de machismo”, compartió una fuente de las organizaciones más importantes a nivel nacional que trabaja con los votantes latinos.', 'language': 'es', 'crawl_date': '2020-01-01T02:39:37+00:00'}]\n"
     ]
    }
   ],
   "source": [
    "with open('sample_data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Print the data\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings added to the dataset.\n"
     ]
    }
   ],
   "source": [
    "# Extract 'plain_text' field\n",
    "plain_texts = [article.get(\"plain_text\", \"\") for article in data]\n",
    "\n",
    "# Call the generate_embeddings function\n",
    "# Ensure you have a `client` instance properly initialized before this step\n",
    "try:\n",
    "    embeddings = generate_embeddings(plain_texts)\n",
    "except Exception as e:\n",
    "    print(f\"Error generating embeddings: {e}\")\n",
    "    embeddings = []\n",
    "\n",
    "# Add embeddings back to the dataset\n",
    "for article, embedding in zip(data, embeddings):\n",
    "    article[\"embedding\"] = embedding\n",
    "\n",
    "# Optionally, save the updated data back to a file\n",
    "with open(\"your_file_with_embeddings.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Embeddings added to the dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IterableDataset({\n",
       "    features: ['requested_url', 'plain_text', 'published_date', 'title', 'tags', 'categories', 'author', 'sitename', 'image_url', 'language', 'language_score', 'responded_url', 'publisher', 'warc_path', 'crawl_date'],\n",
       "    num_shards: 76\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import faiss\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 1: Load Articles from Hugging Face Dataset\n",
    "def load_and_filter_articles():\n",
    "    \"\"\"\n",
    "    Load articles from the CCNews dataset filtered by language and date.\n",
    "    \"\"\"\n",
    "    # Load dataset in streaming mode\n",
    "    dataset = load_dataset(\"stanford-oval/ccnews\", name=\"2020\", streaming=True)\n",
    "    \n",
    "    # Define filter criteria\n",
    "    def filter_function(article):\n",
    "        # Filter by language and crawl date\n",
    "        return (\n",
    "            article[\"language\"] in [\"en\", \"es\"] and \n",
    "            \"2020-11\" in article[\"crawl_date\"] and\n",
    "            \"election\" in (article.get(\"title\", \"\").lower() + article.get(\"content\", \"\").lower())\n",
    "        )\n",
    "    \n",
    "    # Apply the filter\n",
    "    filtered_articles = []\n",
    "    for article in tqdm(dataset[\"train\"], desc=\"Filtering Articles\"):\n",
    "        if filter_function(article):\n",
    "            filtered_articles.append(article)\n",
    "    \n",
    "    return filtered_articles\n",
    "\n",
    "# Step 2: Set Up M2-BERT-80M-8K-Retrieval\n",
    "def setup_model():\n",
    "    \"\"\"\n",
    "    Load the M2-BERT model and tokenizer.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"together-ai/M2-BERT-80M-8K-Retrieval\")\n",
    "    model = AutoModel.from_pretrained(\"together-ai/M2-BERT-80M-8K-Retrieval\")\n",
    "    return tokenizer, model\n",
    "\n",
    "# Step 3: Generate Embeddings\n",
    "def embed_text(text, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a given text using M2-BERT.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=8000, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use the [CLS] token embedding (first token)\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "\n",
    "def generate_embeddings(articles, tokenizer, model, batch_size=16):\n",
    "    \"\"\"\n",
    "    Generate embeddings for all filtered articles using batching.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    batch_texts = []  # To store text for batching\n",
    "    \n",
    "    for article in tqdm(articles, desc=\"Preparing Batches\"):\n",
    "        text = article.get(\"title\", \"\") + \" \" + article.get(\"content\", \"\")\n",
    "        batch_texts.append(text)\n",
    "        \n",
    "        # If batch is full, process it\n",
    "        if len(batch_texts) == batch_size:\n",
    "            inputs = tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, max_length=8000, padding=True)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            # Extract [CLS] token embeddings for the batch\n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings.append(batch_embeddings)\n",
    "            batch_texts = []  # Reset batch\n",
    "\n",
    "    # Process remaining texts (if any)\n",
    "    if batch_texts:\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, max_length=8000, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        embeddings.append(batch_embeddings)\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Step 4: Set Up FAISS Vector Store\n",
    "def setup_faiss(embeddings):\n",
    "    \"\"\"\n",
    "    Create and populate a FAISS vector store.\n",
    "    \"\"\"\n",
    "    dimension = embeddings.shape[1]  # Embedding dimension\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "# Step 5: Query and Retrieve\n",
    "def retrieve_documents(query, tokenizer, model, index, articles, k=5):\n",
    "    \"\"\"\n",
    "    Retrieve top-k documents for a query.\n",
    "    \"\"\"\n",
    "    query_embedding = embed_text(query, tokenizer, model)\n",
    "    distances, indices = index.search(query_embedding[np.newaxis, :], k)\n",
    "    retrieved_docs = [articles[i] for i in indices[0]]\n",
    "    return retrieved_docs\n",
    "\n",
    "# Step 6: Use Retrieved Documents in a RAG Pipeline\n",
    "def generate_response(query, retrieved_docs, generator_model):\n",
    "    \"\"\"\n",
    "    Generate a response using retrieved documents and a text generator.\n",
    "    \"\"\"\n",
    "    context = \"\\n\".join([doc.get(\"title\", \"\") + \" \" + doc.get(\"content\", \"\") for doc in retrieved_docs])\n",
    "    prompt = f\"Query: {query}\\nContext: {context}\\nAnswer:\"\n",
    "    \n",
    "    # Generate the response (replace 'your-generator-model' with the actual generator model)\n",
    "    response = generator_model(prompt, max_length=512)\n",
    "    return response[0][\"generated_text\"]\n",
    "\n",
    "# Step 7: Main Pipeline\n",
    "def main():\n",
    "    # Load and filter articles\n",
    "    print(\"Loading and filtering articles...\")\n",
    "    articles = load_and_filter_articles()\n",
    "\n",
    "    # Set up model\n",
    "    print(\"Setting up model...\")\n",
    "    tokenizer, model = setup_model()\n",
    "\n",
    "    # Generate embeddings\n",
    "    print(\"Generating embeddings...\")\n",
    "    embeddings = generate_embeddings(articles, tokenizer, model, batch_size=16)\n",
    "\n",
    "    # Set up FAISS index\n",
    "    print(\"Setting up FAISS vector store...\")\n",
    "    index = setup_faiss(embeddings)\n",
    "\n",
    "    # Example query\n",
    "    query = \"What happened in the 2020 US Presidential Election?\"\n",
    "    print(f\"Query: {query}\")\n",
    "\n",
    "    # Retrieve documents\n",
    "    print(\"Retrieving documents...\")\n",
    "    retrieved_docs = retrieve_documents(query, tokenizer, model, index, articles)\n",
    "\n",
    "    # Generate response\n",
    "    print(\"Generating response...\")\n",
    "    from transformers import pipeline\n",
    "    generator_model = pipeline(\"text-generation\", model=\"together-ai/your-generator-model\")\n",
    "    response = generate_response(query, retrieved_docs, generator_model)\n",
    "\n",
    "    print(\"\\nGenerated Response:\")\n",
    "    print(response)\n",
    "\n",
    "# Run the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
